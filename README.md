### This is a PyTorch implementation of the paper: [Learn to aggregate global and local representations for few-shot learning](https://link.springer.com/article/10.1007/s11042-023-14413-1)

![ScreenShot](/images/framework.png)

## Requirements

* Pytorch 1.4.0
* Torchvision 0.5.0
* At least 1 GPU

## Datasets

* [miniImageNet](https://drive.google.com/open?id=0B3Irx3uQNoBMQ1FlNXJsZUdYWEE)
* [tieredImageNet](https://drive.google.com/open?id=0B3Irx3uQNoBMQ1FlNXJsZUdYWEE)
* [Stanford Dogs](http://vision.stanford.edu/aditya86/ImageNetDogs/)
* [Stanford Cars](https://ai.stanford.edu/~jkrause/cars/car_dataset.html)
* [CUB](http://www.vision.caltech.edu/visipedia/CUB-200.html)

## Citation
If you find our code useful for your research, please consider citing it using the bibtex:

```
@article{abdelaziz2023learn,
  title={Learn to aggregate global and local representations for few-shot learning},
  author={Abdelaziz, Mounir and Zhang, Zuping},
  journal={Multimedia Tools and Applications},
  pages={1--24},
  year={2023},
  publisher={Springer}
}
```
## References 

* [DN4](https://github.com/WenbinLee/DN4)
* [MML](https://github.com/chenhaoxing/M2L)
